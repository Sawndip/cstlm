
\subsection{Kneser Ney Language Modelling}
\label{sec-lm}

%Background, mathematical formulation.

This paper presents an efficient \ngram language model (LM) using succinct indexes to encode the model's sufficient statistics. % in such a way that they can be stored using only a small memory footprint and accessed efficiently. 
Although our method is generally applicable to many LM variants, 
we focus on the Kneser-Ney LM~\cite{kneser1995improved}, 
specifically the interpolated variant described in \newcite{chen1996empirical}, 
which has been shown to outperform other \ngram LMs and has become the de-facto standard.

Interpolated Kneser-Ney describes the conditional probability of a word $w_i$ conditioned on the context of $m-1$ preceeding words, $w_{i-m+1}^{i-1}$, as 
\begin{align}
P(w_i |& w_{i-m+1}^{i-1})=\frac{\max\left[c(w^{i}_{i-m+1}) - D_m,0\right]}{c(w^{i-1}_{i-m+1})} \nonumber \\
& \hspace{-6mm} +\frac{D_m \nlplus{w^{i-1}_{i-m-1}\Bigcdot} }{c(w^{i-1}_{i-m+1})}  
\bar{P}(w_i | w_{i-m+2}^{i-1}) ,  
\label{eq:high}
\end{align}
where lower-order smoothed probabilities are defined recuresively (for $1<k<m$) as: 
\begin{align}
\! \bar{P}(w_i |& w_{i-k+1}^{i-1})
 = \frac{\max\left[\nlplus{\Bigcdot w_{i-k+1}^{i}} - D_k,0\right]}{\nlplus{\Bigcdot w_{i-k+1}^{i-1} \Bigcdot}} \nonumber \\
&+ \frac{D_k \nlplus{w_{i-k+1}^{i-1}\Bigcdot}}{\nlplus{\Bigcdot w_{i-k+1}^{i-1} \Bigcdot}} \bar{P}(w_i | w_{i-k+2}^{i-1}) \,  . \! \label{eq:mid}
\end{align}
In the above formula, $D_k$ is the $k$gram-specific discount parameter, and 
the \emph{occurrence count} 
\mbox{$\nlplus{\uu\Bigcdot} = |\{w: c(\uu w)>0\}|$} 
is the number of observed word types following the pattern $\uu$; 
the occurence counts $\nlplus{\Bigcdot\uu}$ and $\nlplus{\Bigcdot\uu\Bigcdot}$ 
are defined accordingly.
%The highest order probability estimate in (\ref{eq:high}) interpolates a discounted frequency estimate with a $m-1$ order probability, defined as 
%Note the difference between (\ref{eq:mid})~and~(\ref{eq:high}), in that the frequency counts are redefined as occurrence counts. 
%The above equation is self-recursive, where each step shrinks the conditioning context by the least recent symbol. 
The unigram probabilities are defined as
%\begin{align*}
%\label{eq:low}
%
$ \bar{P}(w_i) = \nlplus{\Bigcdot w_i} / \nlplus{\Bigcdot\Bigcdot}$.

%\end{align*}
%\newcite{chen1996empirical} report maximum likelihood estimates for the discount parameters, as simple functions of the corpus statistics for \ngrams occuring 1--4 times or occuring in 1--4 unique left contents. 
% Note that many other LM methods are defined in a similar manner to interpolated Kneser-Ney, using similar counts and occurrence counts for \ngram patterns.
Modified Kneser-Ney, proposed by~\newcite{chen1996empirical}, typically outperforms interpolated Kneser-Ney by using context specific discount parameters.
%
Its implementation with our data structures is straightforward in principle, 
but brings a few added complexities in terms of dynamic computing of 
new types of occurrence counts, which we leave for future work.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "cstlm"
%%% End: 
