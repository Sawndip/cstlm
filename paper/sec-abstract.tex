In this paper, we build  fast and compact $n$gram language models (LMs)  on web-scale corpora using modern succinct data structures.
%
We show how to compactly index the text statistics needed by Kneser-Kney (KN) LMs 
to efficiently make the smoothed probabilities on the fly.
%
Importantly, the stored index and text statistics are kept intact for $n$gram LMs with different orders, 
hence allow to relax the Markov assumption made in these LMs by considering very large contexts.
%
At the core, the stored index make use of compressed suffix structures including FM-index and wavelet trees.
%
Our comprehensive experiments show that the memory footprint of our high-order KN-LMs on web-scale corpora, which are infeasible to build using 
state-of-the-art LM toolkits due to memory and construction time constraints, is roughly the same as that for low-rder LMs while maintaining 
a reasonable query time. 
