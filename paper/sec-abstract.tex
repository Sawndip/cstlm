Efficient methods for storing and querying language models are critical for scaling to large corpora and high Markov orders.
%While most previous work has focussed on efficient means of indexing \ngrams using a fixed, low Markov order,
In this paper we propose methods for modelling extremely large corpora without
imposing a Markov condition.
At its core, our approach uses a succinct index -- a \emph{compressed suffix tree} -- which provides near optimal compression while supporting efficient search.
We present algorithms for on-the-fly computation of probabilities under a
%n infinite order
Kneser-Ney language model.
Our technique is \emph{exact} and although slower than leading LM
toolkits, it shows promising scaling properties,
which we demonstrate \ehsan{Do we actually show this?} through $\infty$-order modelling over the full Wikipedia collection. 


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "cstlm"
%%% End: 
