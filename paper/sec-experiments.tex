
\subsection{Datasets}
We used Europarl dataset and the data was numberized after tokenizing, splitting, and excluding xml markups. The first $10K$ sentences were used as the test data, and the last 80\% as the training data (See Table~\ref{fig:data}).

\begin{table}
\resizebox{1\columnwidth}{!}{
\begin{tabular}{ll|cc|c}
Language&&Size (MB)&Tokens (M)& Sentences (K)\\
\toprule 
Bulgarian&BG&36.11&8.53&329\\
Czech&CS&53.48&12.25&535\\
German&DE&171.80&44.07&1785 \\
English&EN&179.15&49.32&1815\\
Finnish&FI&145.32&32.85&1737\\
French&FR&197.68&53.82&1792\\
Hungarian&HU&52.53&12.02&527\\
Italian&IT&186.67&48.08&1703\\
Portuguese&PT&187.20&49.03&1737\\
\end{tabular}}
\caption{Tokens and sentence counts refer to the training partition. \trevor{Move to \supp}}\label{fig:data}
\end{table}
Figure~\ref{fig:data} is showing XXX.
\begin{figure}
\includegraphics[width=\columnwidth]{figures/german_pattern_size.pdf}
\caption{Number of successful queries across different pattern sizes from KN computation over the German test set, with unbounded $m$.}
\end{figure}\label{fig:germanpattern}

\subsection{Perplexity Evaluation}
We evaluated the perplexity across different languages and using different order $m$-grams varying from $2$ to $\infty=99999$. While matching SRILM in perplexity we compared our time and memory usage during the training and query time with SRILM default (optimized for time), and compact(optimized for space). Figure~\ref{figure:pplx} shows the gain in perplexity with respect to $m$. 
\begin{figure}
\input{figures/fig-europarl-pplx-ngram}
\caption{All 8 languages pplx results with m=2..10,15,20,infinity(=99999).}
\end{figure}\label{figure:pplx}
We speculate that the drop in higher order perplexity gain is due using to Kneser-Ney discounting which is not designed for higher order $m$-gram language models and starts to heavily discount after $5-gram$. We leave further exploration of richer techniques such as Modified Kneser-Ney, or Sequence Memoizer to our future work. 
\subsection{Time-Memory}
Figure~\ref{figure:time-space} ($\log10-\log10$) compares our timing and memory usage with SRILM on German part of the Europarl dataset and highlights the merits of our method both in construction and query time. In construction time, while both of our approaches are independent of the order of $m$gram, our fixed construction time outperforms both SRILM methods in terms of memory for $m$grams of $5$ and higher orders. In terms of the timing, we outperform SRILM compact after bigram, and SRILM default for $m$grams of $3$ and higher orders.

In query time, our memory footprint is constant and much lower that SRILM compact, and default after $3$, and $2$ respectively. While in terms of timing, the single CST method is $300x$ slower compared to SRILM, but reduces the speed gap to $12x$.  

Another observation is the computational cost of computing $\nlplus{\Bigcdot w_{i-k+1}^{i-1} \Bigdot}$ when moving from $2$-gram, to $3$-gram which affects the timing. As mentioned earlier, this quantity is not required unless we move to $3$ or higher order $m$grams.

\begin{figure}
\includegraphics[width=\columnwidth]{figures/Time-Space.pdf}
\caption{On German, showing the size in MB (y axis) and time (x axis) for 4 methods: CST single, CST dual, SRILM, SRILM compact.}
\end{figure}\label{figure:time-space}
%\missingfigure[figwidth=\columnwidth]{Wikipedia pplx (right axis), and time (left axis) for the single CST on characters vs words as a function of \ngram size.}

%\missingfigure[figwidth=\columnwidth]{Wikipedia histogram over \ngram size, perhaps as a figure or table?}

%\missingfigure[figwidth=\columnwidth]{Wikipedia plot (stacked bar?) of time spent in each method (n1+ x 3; backward search etc) as a function of n.}






%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "cstlm"
%%% End: 
