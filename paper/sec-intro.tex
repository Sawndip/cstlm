%Language modelling is the task of estimating the probability of sequences of words in a language.
Language models (LMs) are critical components in many modern NLP systems, including machine translation \cite{koehn2010book} and automatic speech recognition \cite{rab93book}.
%
%One approach for accurate language modeling is neural 
%\cite{Bengio:2003:NPL,DBLP:conf/interspeech/MikolovKBCK10}, but
%another is  
The most widely used LMs are \ngram models \cite{chen1996empirical},
based on explicit storage of \ngrams and their counts, 
%based on a simple frequency counts, and 
which have proved highly accurate when trained on large datasets.
%
%The predominant approach to count-based language modelling is the $n$gram model,
%where a typical LM can contain as many as several hundred billions of $n$grams XX.
%
To be useful, LMs need to be not only accurate but also fast and compact.
% This has motivated considerable research into space and time efficent structures and algorithms
% for storing and querying language models.
% %
% However these methods which are largely based on explicitly storing \ngram counts do not scale well to large $m$.
% In this paper, we propose a  fast and compact method for implementing \ngram LMs
% %, the predominant approach in count-based language modeling, 
% using modern succinct data structures.
% Our technique scales well with $m$, even allowing non-Markov language modelling with unbounded conditioning context.

%where the probability of a sequence of words is decomposed based on the chain rule ad approximatedusing the Markov assumption XX. 
%A typical count-based LM can contain as many as several hundred billions of $n$grams
%In this paper, we build  fast and compact $n$gram LMs, the predominant approach in count-based language modeling, on web-scale corpora using modern succinct data structures.
%
%We show how to compactly index the text statistics needed by $n$gram language models, so that they can be queried efficiently to make the probabilities on the fly.       
%
%Importantly, the stored index and text statistics are kept intact for $n$gram LMs with different orders (i.e. the context size), hence allow to relax the
%Markov assumption made in these LMs by considering very large contexts.



Depending on the order and the training corpus size, a typical \ngram
LM may contain as many as several hundred billions of \ngrams
\cite{brants2007large},
raising challenges of efficient storage and retreival.
% Trevor: raising important algorithmic questions regarding efficient storage and retreival
%
As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate  \emph{lossy} LMs 
\cite{Chazelle:2004:BFE:982792.982797,DBLP:conf/acl/TalbotO07,guthrie2010storing},
or \emph{loss-less} LMs 
backed by tries \cite{stolcke2011srilm},
or related compressed structures 
\cite{Germann:2009:TPT:1621947.1621952,heafield2011kenlm,pauls2011faster,sall11,DBLP:conf/acl/WatanabeTI09}.
% Trevor: sentence needed on LREC paper -- the approach most similar to ours
% Trevor: and also SRI/Ken
However, none of these approaches scale well to very high-order $m$
or very large corpora, due to their high memory and time requirements.
An important exception is
\newcite{kennington2012suffix}, who also propose a language model
based on a suffix tree which scales well with $m$ but poorly with the corpus size (requiring memory of
about $20\times$ the training corpus).
% Our approach also develops a LM using suffix trees, albeit modern
% compressed structures which occupy a much smaller memory footprint, reducing the memory 
% requirement from $20\times$ to $\frac{1}{2}$ of the uncompressed
% corpus size, making the approach feasible for all but the largest
% modern corpora.


%
%However, none of the state-of-the-art LM toolkits provides the opportunity of
%investigating the behaviour of high-order LMs  
% 
% Being able to work with high-order  LMs and investigate their behaviour 
% with different smoothing techniques on large corpora is important, but 
% infeasible with current state-of-the-art LM toolkits\footnote{ 
% %none of the state-of-the-art toolkits provides this opportunity\footnote{
% Indeed, there is clearly evidence that training high-order \ngram
% LMs with the right amount of data and smoothing technique is beneficial \cite{wood2011sequence}.
% }. 


%In $n$gram LMs, the probability of a sequence of words is decomposed based on the chain rule 
%and approximated using the Markov assumption 
%$P(w_1,\ldots,w_{\ell}) \approx \prod_{i=1}^{\ell} P(w_i|w_{i-n+1}^{i-1})$. 
%
%The Markov assumption reduces the number of model parameters 
%(which translates to more robust parameter estimation and smaller storage) 
%at the cost of probabililty approximation due to strong independence assumptions. 
%
%Making the space requirements  and query time of high-order LMs reasonable,
%our approach leads to weakening the strong independence assumptions inherent in $n$gram LMs.
%
%It is well known that, due to power-law nature of natural language [Zipf], maximum likelihood 
%estimator significantly overfits the training corpora. 
%
%Therefore, we build the indices to efficiently retrieve the counts 
%our indices store raw frequency counts 
%needed to compute \emph{smoothed} $n$gram probabilities using Kneser-Kney (KN) smoothing, 
%a powerful technique with elegant statistical properties\footnote{Although 
%we focus on KN-smoothing in this paper, our approach can be used to index and retrieve 
%counts needed in other smoothing techniques as well [chen and goodman].} [kneser kney].  
 
In contrast, we make use of recent advances in  \emph{compressed suffix trees} (\CSTs) \cite{rno-talg11} 
to build compact indices with much more modest memory requirements (about
$\frac{1}{2}$ the corpus size).
We present methods for extracting frequency and unique occurrence count
statistics for \ngram queries from \CSTs,  and two algorithms for
computing Kneser-Ney LM probabilities on the fly using these statistics.
The first method uses two \CSTs (over the corpus and the reversed corpus),
% one over the corpus and another over the
%reversed corpus, 
which allow for efficient computation of the number of unique contexts to the left
and right of an \ngram, 
but is inefficient in several ways, most notably when computing the
number of unique contexts to both sides.
% \ngram is more difficult to compute, as is the process of searching in both \CSTs,
% resulting in slow runtime performance. 
Our second method addresses this problem using a single \CST backed by
a FM-index \cite{fmmn-talg07}, which results in a better time
complexity of querying and a considerably faster runtime.
 
 % We further show how to leverage count queries for shorter contexts in queries for 
% longer contexts to speed up the computation of KN-smoothed probabilities on the fly 
% (\S\ref{sec-dual-cst}).
% %
% Our second proposal consists of only one % \emph{augmented} 
% \CST, which further improves both the space an
Our experiments show that our method is practical for large-scale language
modelling, although querying is substantially slower than a \SRILM benchmark.
However our technique scales much more gracefully with Markov order $m$, allowing
unbounded `non-Markov' application, and enables training on large
corpora as we demonstrate on the complete Wikipedia dump.
% What's the raw text size before numberising Matthias?
Overall this paper illustates the vast potential succinct indexes have for language modelling  and
other `big data' problems in language processing such as machine translation 
\cite{zhang_vogel_2005,DBLP:conf/acl/Callison-BurchBS05,lopez2008,DBLP:journals/tacl/HeLL15}.

% with Kneser-Kney (KN) smoothing,  
% a powerful technique with elegant statistical properties\footnote{Although
% we focus on KN-smoothing, our approach can be used to index and compute
% counts needed in other smoothing techniques as well \cite{chen1996empirical}.}  
% \cite{kneser1995improved}.
% % 
% The key quantities for computing probabilities under KN-LMs
% are raw frequencies of \ngrams 
% and occurrence counts, i.e. the number of different contexts 
% in which an \ngram has 
% occurred whose efficient computation is challenging. 
% %
% %The raw frequency counts are simple to compute, but occurrence counts 
% %are challenging. 
% %
% Our first proposal consists of two \CSTs to compute count queries 
% in KN-LM (\S\ref{sec-lmsdsl}). 
% % 
% We further show how to leverage count queries for shorter contexts in queries for 
% longer contexts to speed up the computation of KN-smoothed probabilities on the fly 
% (\S\ref{sec-dual-cst}).
% %
% Our second proposal consists of only one % \emph{augmented} 
% \CST, which further improves both the space and query-time requirements
% of our first proposal (\S\ref{sec-single-cst}). 
% % 

%Experiments ....
% Trevor: the experiments show that this approach matches the results of SRILM, while consuming a static memory footprint and construction cost irrespective of the \ngram order
% Trevor: futher, this allows us to run with unlimited markov order, which while slower than optimised low order models like \SRILM augurs well for future developments.
% Trevor: illustrate the scaling benefits of the model by building a character level LM over wikipedia, in only a few hours and modest memory footprint of a few GB
% Trevor: this work serves as a proof of concept, namely that high order language modelling is possible, and suitable for further optimisation to yield transformative tools for natural language processing.

%The predominant approach to count-based language modelling is the $n$gram model, where a typical LM can contain as many as several hundred billions of $n$grams XX.

%A typical LM can contain as many as severl hundred billions of $n$gramsFor modern NLP systems 
%The predominanr approach in language modelling is the $n$gram model, where the probability of a sequence of words is decomposed based on the chain rule and
%approximated using the Markov assumption XX. 
%

%The dominant approach to language modelling is $n$gram  model, where 

%LMs important, critical aspects are speed and scale. One avenue of better LMs using NN; another
%better count based LMs to harness more data (e.g., stupid backoff). 

%We're the latter, but marrying large scale with KN smoothing (better method). Crux is how to compute
%the quantities (counts, occ counts). 

%Suffix trees / arrays / tries etc used in the past. Either precomputed (KLM/SRI) or on the fly (slow
%LREC paper). But doesn't scale. We propose compressed SA/ST methods, which scale to massive datasets.

%Brief outline of how the method works; using one or two suffix trees. Forward/backward search.

%Pata on experiments: pplx good; can scale to large n and massive corpora; adequate runtime
%performance, around 100x slower than SRILM. Reranking gains? Can we run with INFINITE order?

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "cstlm"
%%% End: 
