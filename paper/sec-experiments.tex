
\paragraph{Datasets}
We used Europarl dataset and the data was numberized after tokenizing, splitting, and excluding xml markups. The first $10K$ sentences were used as the test data, and the last 80\% as the training data, giving rise to training corpora of between 8M and 50M tokens and binary encoding of word integers of up to 200MB (see \supp Table~4).
\trevor{Sentence on wikipedia}

%\begin{table}
%\resizebox{1\columnwidth}{!}{
%\begin{tabular}{ll|cc|c}
%Language&&Size (MB)&Tokens (M)& Sentences (K)\\
%\toprule 
%Bulgarian&BG&36.11&8.53&329\\
%Czech&CS&53.48&12.25&535\\
%German&DE&171.80&44.07&1785 \\
%English&EN&179.15&49.32&1815\\
%Finnish&FI&145.32&32.85&1737\\
%French&FR&197.68&53.82&1792\\
%Hungarian&HU&52.53&12.02&527\\
%Italian&IT&186.67&48.08&1703\\
%Portuguese&PT&187.20&49.03&1737\\
%\end{tabular}}
%\caption{Tokens and sentence counts refer to the training partition. \trevor{Move to \supp}}\label{fig:data}
%\end{table}

\begin{figure}[tb]
\input{figures/fig-europarl-pplx-ngram}
\caption{Perplexity results on several Europarl languages for different \ngram sizes, $m=2\ldots10,15,20,\infty$.}
\label{fig:pplx}
\end{figure}


\paragraph{Perplexity}
We evaluated the perplexity across different languages and using \ngrams of varying order from $m\in[2,\infty)$ (unbounded), as shown on Figure~\ref{fig:pplx}.
Our results matched the perplexity results from \SRILM (for smaller values of $m$ in which \SRILM training was feasible, $m \le 10$).
Note that perplexity drops dramatically from $m=2\ldots5$ however the gains thereafter are modest for most languages.
Despite this, several large \ngram matches were found as illustrated in Figure~\ref{fig:germanpattern}, ranging in size up to a 34-gram match.
We propose that the perplexity plateau is due to the simplistic Kneser-Ney discounting formula which is not designed for higher order \ngram LMs and appear to discount large \ngrams too aggressively. 
%We leave further exploration of richer discounting techniques such as Modified Kneser-Ney \cite{chen_goodman} or the Sequence Memoizer \cite{wood_teh} to our future work.
Richer discounting techniques such as Modified Kneser-Ney \cite{chen1996empirical} or the Sequence Memoizer \cite{wood2011sequence} would be able to better exploit these large contexts.


\begin{figure}[tb]
\includegraphics[width=\columnwidth]{figures/german_pattern_size.pdf}
\caption{Number of \ngrams of different sizes in both training and test sets for Europarl German}
%\caption{Number of successful queries across different pattern sizes from KN computation over the German test set, with unbounded $m$.}
\label{fig:germanpattern}
\end{figure}

\begin{figure}[tb]
\input{figures/fig-eu-de-space-time}
\caption{Time versus space tradeoffs measured on Europarl German (de) dataset, showing memory and time requirements for the four methods: CST single, CST dual, \SRILM, and \SRILM compact. Times were measured using a 10k sentence test set.}
\label{fig:spacetime}
\end{figure}

\paragraph{Time-Memory}
Figure~\ref{fig:spacetime} ($\log-\log$) compares our timing and memory usage with \SRILM on German part of the Europarl dataset and highlights the merits of our method both in construction and query time. In construction time, while both of our approaches are independent of the order of $m$gram, our fixed construction time outperforms both \SRILM methods in terms of memory for orders $m \ge 5$. 
In terms of time, we outperform \SRILM compact beyond bigrams, and \SRILM default for $m \ge 3$. 
For querying, our memory footprint is constant and much lower that \SRILM compact and \SRILM default after $m=3$, and $m=2$ respectively. 
The results are less impressive for query times, in which the single CST method is $300x$ slower compared to \SRILM for $2$-gram, but shows better scaling, with the gap reducing to $12x$ worse by $10$-grams.  
Also observe the high the computational cost of computing $\nlplus{\Bigcdot \alpha \Bigcdot}$, which is apparent when moving from $m=2$ to $m=3$, the first point where this method is called. 
This is an obvious candidate for optimisation, which could radically improve query times.
%\footnote{Preliminary results show that query times can be reduced dramatically, to roughly match \SRILM. For space reasons we defer this for future work.} 
%As mentioned earlier, this quantity is not required unless we move to $3$ or higher order $m$grams.





%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "cstlm"
%%% End: 

