
\subsection{Kneser Ney}

%Background, mathematical formulation.

This paper presents an efficient $n$gram language model (LM) using compressed index structures to encode the model's sufficient statistics in such a way that they can be stored using only a small memory footprint and accessed efficiently. 
We focus on the Kneser-Ney LM~\cite{kneser1995improved}, which has been shown to outperform other $n$gram models~\cite{chen1996empirical} and has become a de facto standard for most language modelling applications.
We now provide an overview of the Kneser-Ney LM and the nature of its sufficient statistics, which drive the algorithms in the following section.

\trevor{I've dropped this from the ShareLatex paper. It needs more pruning and focusing to the goals of the paper. I.e., less of a focus on what could be done, more on what is done / we have used.}

% from share-latex draft
% needs cleaning up / general editing -- particularly to make it
% specific to the point of the paper, Kneser Ney smoothed ngram LMs.

While the probability of a sequence is usually calculated as a product of conditional terms in Eq.~\ref{conditional}, there are certain optimizations that we need to make for: (I) handling unseen sequences in the test time, and (II) dealing with the sparsity problem, these conditional terms are often calculated using a smoothing technique where the probability of each n-gram is smoothed by using the statistics of the lower order n-grams, \emph{backoff}, or \emph{interpolation}.
\begin{align}
P(w_{i}|w_{i-n+1}^{i-1}) &= \alpha(w_{i}|w_{i-n+1}^{i-1})\\\nonumber
&+\gamma(w_{i-n+1}^{i-1})P(w_{i}|w_{i-n+2}^{i-1})
\end{align}
where the term $\alpha(.)$ is calculated using maximum likelihood estimation of the n-gram, or its refinement by deduction some discounts. The most well-known smoothing technique used for language modelling is Kneser-Ney interpolation which was originally proposed in~\cite{kneser1995improved} as a backoff model, and~\newcite{chen1996empirical} transformed that into the following interpolative model:
\begin{align}
&P_{KN}(w_{i}|w_{i-n+1}^{i-1}) = \frac{\max\{c(w_{i-n+1}^{i} - D,0\}}{c(w_{i-n+1}^{i-1})}\\\nonumber
&+ \frac{D N_{1+}(w_{i-n+1}^{i-1}\Bigcdot)}{c(w_{i-n+1}^{i-1})} P_{KN}(w_{i}|w_{i-n+2}^{i-1})
\end{align}
where $N_{1+}(w_{i-n+1}^{i-1}\Bigcdot)$ is the continuation count indicating the number of word types that can complete a sequence, $D$ is the discount parameter which can be estimated using a held-out data, or set to $D=\frac{n_1}{n_{1}+2 n_{2}}$ where $n_x$ is the total number of n-grams with exactly $x$ counts. In lower order n-grams instead of the actual counts the continuation counts are used and this backing-off stops at the unigram level. A modification of Kneser-Ney, proposed by ~\cite{chen1996empirical}, is proved to be more effective which instead of a single discout parameter, discounts will be chosen depending on the actual count of the querried sequence.

% this paragraph belongs in related work section
While the state-of-the-art performance is reached by using Kneser-Ney,
and its refinement, most of the effort in the recent years have been
dedicated in exploring data structures and hashing techniques for
storing the data, or the sufficient statistics for language models. 

%\subsection{Kneser-Ney Interpolation}
Calculating the probability of the pattern $w^{1}_{3}$ via Kneser-Ney, assuming the 3-gram model, is straightforward. For the highest order ngram we use the following formula:
\begin{align}\label{highestorder}
 P_{KN}(w_3|w^{1}_{2}) &= \frac{\max\{c(w^{1}_{3}) - D,0\}}{c(w^{1}_{2})}\\\nonumber
 &+ \frac{D*N_{1+}(w^{1}_{2}\Bigcdot)}{c(w^{1}_{2})} P_{KN}(w_3|w_2)
\end{align}
Where for the lower order ngrams, i.e. $w^{2}_{3}$, the same formula is being used but this time instead of the actual counts we use the countinuation counts:
\begin{align}\label{midorder}
P_{KN}(w_3|w_2) &= \frac{\max\{N_{1+}(\Bigcdot w^{2}_{3}) - D,0\}}{N_{1+}(\Bigcdot w_2 \Bigcdot)} \\\nonumber
&+ \frac{D*N_{1+}(w_2\Bigcdot)}{N_{1+}(\Bigcdot w_2 \Bigcdot)}*P_{KN}(w_3)
\end{align}
And for unigram, i.e. $w_3$, we use the similar formula:
\begin{align}\label{lowestorder}
P_{KN}(w_3) &= \frac{\max\{N_{1+}(\Bigcdot w_3) - D,0\}}{N_{1+}(\Bigcdot \Bigcdot)} \\\nonumber 
&+ \frac{D*N_{1+}(\Bigcdot)}{N_{1+}(\Bigcdot\Bigcdot)} P_{KN}^{0}
\end{align}
where the zeroth-rder distribution is a uniform distribution:
\begin{align}\label{uniform}
P_{KN}^{0} = \frac{1}{|Vocab|}
\end{align}
since $|Vocab| = N_{1+}(\Bigcdot)$ we can simplify Eq.\ref{lowestorder} as:
\begin{align}\label{lowestordersimple}
P_{KN}(w_3) = \frac{N_{1+}(\Bigcdot w_3)}{N_{1+}(\Bigcdot\Bigcdot)}
\end{align}
the discount parameter $D$ is calculated as $D=\frac{n_1}{n_1 + 2 n_2}$, where $n_1,\ n_2$ are the total number of n-grams (trigrams in this case) with exactly one and two counts in the training data, and $N_{1+}(w_2\Bigcdot)$ is the number of word-types that can appear after \emph{$w_2$}: $N_{1+}(w_2\Bigcdot) = |\{w_i: c(w_2w_i)>0\}|$.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "cstlm"
%%% End: 
