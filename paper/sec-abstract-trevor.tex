Efficient means for storing and querying langugage models are critical for scaling  to large text corpora and high Markov orders.
While most previous work has focussed on efficient means of indexing \ngrams using a fixed, low Markov order,
in this paper we develop techniques that allow modelling of extremely large corpora without imposing a Markov condition.
Our technique leverages succinct indexes in the form of compressed suffix trees which provide near optimal compression while still allowing efficient search.
We present algorithms for on-the-fly computation of probabilities under an infinite order Kneser-Ney language model.
Our technique is \emph{exact} and although slower than leading LM toolkits, such as \SRILM, shows promising scaling properties, which we demonstrate through $\infty$ order modelling over the full Wikipedia collection. 
% show this is efficient
% proof of concept, still one or two orders of magnitude worse than SRILM -- scales better




%In this paper, we build  fast and compact high-order \ngram language models (LMs) using modern succinct data structures.
%We show how to compactly index the text statistics needed by Kneser-Kney (KN) LMs 
%to efficiently make the smoothed probabilities on the fly.
%Importantly, the stored index for text statistics is kept intact for \ngram LMs with different orders, 
%which allows to relax the Markov assumption made in these LMs by considering very large contexts.
%
%At the core, the stored index makes use of compressed suffix structures including FM-index and wavelet trees.
%
%Our comprehensive experiments show that the memory footprint of our high-order KN-LMs, which are infeasible to build using 
%state-of-the-art LM toolkits due to memory constraints, is the same as that for low-order LMs while maintaining 
%a reasonable query-time. 
