
\subsection{Kneser Ney Language Modelling}
\label{sec-lm}

%Background, mathematical formulation.

This paper presents an efficient \ngram language model (LM) using succinct indexes to encode the model's sufficient statistics. % in such a way that they can be stored using only a small memory footprint and accessed efficiently. 
Although our method is generally applicable to many LM variants, we focus on the Kneser-Ney LM~\cite{kneser1995improved}, specifically the interpolated variant described in \newcite{chen1996empirical}, which has been shown to outperform other $n$gram LMs and has become the de-facto standard.

Interpolated Kneser-Ney describes the conditional probability of a word $w_i$ conditioned on the context of $m-1$ preceeding words, $w_{i-m+1}^{i-1}$, as 
\begin{align}
P(w_i |& w_{i-m+1}^{i-1})=\frac{\max\left[c(w^{i}_{i-m+1}) - D_m,0\right]}{c(w^{i-1}_{i-m+1})} \nonumber \\
& \hspace{-6mm} +\frac{D_m \nlplus{w^{i-1}_{i-m-1}\Bigcdot} }{c(w^{i-1}_{i-m+1})}  P(w_i | w_{i-m+2}^{i-1}) ,  
\label{eq:high}
\end{align}
where \mbox{$\nlplus{w^{i-1}_{i-m-1}\Bigcdot} = |\{s: c(w^{i-m}_{i-m-1}s)>0\}|$} is the \emph{occurrence count}, defined as the number of word types observed following the pattern, and $D$ is a vector of \ngram specific discount parameters.
The highest order probability estimate in (\ref{eq:high}) interpolates a discounted frequency estimate with a $m-1$ order probability, defined as 
\begin{align}
\! P(w_i |& w_{i-k+1}^{i-1})
 = \frac{\max\left[\nlplus{\Bigcdot w_{i-k+1}^{i}} - D_k,0\right]}{\nlplus{\Bigcdot w_{i-k+1}^{i-1} \Bigcdot}} \nonumber \\
&+ \frac{D_k \nlplus{w_{i-k+1}^{i-1}\Bigcdot}}{\nlplus{\Bigcdot w_{i-k+1}^{i-1} \Bigcdot}} P(w_i | w_{i-k+2}^{i-1}) \, , \! \label{eq:mid}
\end{align}
for $1<k<m$. 
Note the difference between (\ref{eq:mid})~and~(\ref{eq:high}), in that the frequency counts are redefined as occurrence counts. 
The above equation is self-recursive, where each step shrinks the conditioning context by the least recent symbol. 
The recursion stops with unigram probabilities,
%\begin{align*}
%\label{eq:low}
%
$ P(w_i) = \nlplus{\Bigcdot w_i} / \nlplus{\Bigcdot\Bigcdot}$.
%\end{align*}
%\newcite{chen1996empirical} report maximum likelihood estimates for the discount parameters, as simple functions of the corpus statistics for \ngrams occuring 1--4 times or occuring in 1--4 unique left contents. 
% Note that many other LM methods are defined in a similar manner to interpolated Kneser-Ney, using similar counts and occurrence counts for \ngram patterns.
Modified Kneser-Ney, proposed by~\newcite{chen1996empirical}, typically outperforms interpolated Kneser-Ney by using context specific discount parameters.
The implementation of this with our data structures is straightforward in principle, but brings a few added complexities in terms of dynamic computing other types of occurrence counts, which we leave for future work.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "cstlm"
%%% End: 
