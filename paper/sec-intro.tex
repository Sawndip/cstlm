%Language modelling is the task of estimating the probability of sequences of words in a language.
Language models (LMs) are one of the major and large components in many modern NLP systems, 
e.g. machine translation \cite{koehn2010book} 
and automatic speech recognition \cite{rab93book}.
%
One approach for accurate language modeling is neural 
\cite{Bengio:2003:NPL,DBLP:conf/interspeech/MikolovKBCK10}, but another is  count-based 
\cite{chen1996empirical} to harness the power of more data.
%
%The predominant approach to count-based language modelling is the $n$gram model,
%where a typical LM can contain as many as several hundred billions of $n$grams XX.
%
To be useful, LMs need to be not only accurate but also fast and compact.
%
In this paper, we build  fast and compact high-order \ngram LMs
%, the predominant approach in count-based language modeling, 
using modern succinct data structures.

%where the probability of a sequence of words is decomposed based on the chain rule ad approximatedusing the Markov assumption XX. 
%A typical count-based LM can contain as many as several hundred billions of $n$grams
%In this paper, we build  fast and compact $n$gram LMs, the predominant approach in count-based language modeling, on web-scale corpora using modern succinct data structures.
%
%We show how to compactly index the text statistics needed by $n$gram language models, so that they can be queried efficiently to make the probabilities on the fly.       
%
%Importantly, the stored index and text statistics are kept intact for $n$gram LMs with different orders (i.e. the context size), hence allow to relax the
%Markov assumption made in these LMs by considering very large contexts.



Depending on the order and the training corpus size, a typical \ngram LM may contain as many as several hundred billions of \ngrams \cite{brants2007large},
so storage and query time become challenge.
% Trevor: raising important algorithmic questions regarding efficient storage and retreival
%
As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate  \emph{lossy} LMs 
\cite{Chazelle:2004:BFE:982792.982797,guthrie2010storing},
or \emph{loss-less}  not-small LMs \cite{stolcke2011srilm} where compression and tries 
\cite{Germann:2009:TPT:1621947.1621952,heafield2011kenlm,pauls2011faster}, 
suffix trees \cite{kennington2012suffix}, and LODUS trees 
\cite{sall11,DBLP:conf/acl/WatanabeTI09}  
have been used to further reduce the storage.
% Trevor: sentence needed on LREC paper -- the approach most similar to ours
% Trevor: and also SRI/Ken
However, none of these papers scale up to high-order  
\ngram LMs on large corpora due to their  memory and time constrints when learning the models.
%
%However, none of the state-of-the-art LM toolkits provides the opportunity of
%investigating the behaviour of high-order LMs  
% 
Being able to work with high-order  LMs and investigate their behaviour 
with different smoothing techniques on large corpora is important, but 
infeasible with current state-of-the-art LM toolkits\footnote{ 
%none of the state-of-the-art toolkits provides this opportunity\footnote{
Indeed, there is clearly evidence that training high-order \ngram
LMs with the right amount of data and smoothing technique is beneficial \cite{wood2011sequence}.
}. 

%In $n$gram LMs, the probability of a sequence of words is decomposed based on the chain rule 
%and approximated using the Markov assumption 
%$P(w_1,\ldots,w_{\ell}) \approx \prod_{i=1}^{\ell} P(w_i|w_{i-n+1}^{i-1})$. 
%
%The Markov assumption reduces the number of model parameters 
%(which translates to more robust parameter estimation and smaller storage) 
%at the cost of probabililty approximation due to strong independence assumptions. 
%
%Making the space requirements  and query time of high-order LMs reasonable,
%our approach leads to weakening the strong independence assumptions inherent in $n$gram LMs.
%
%It is well known that, due to power-law nature of natural language [Zipf], maximum likelihood 
%estimator significantly overfits the training corpora. 
%
%Therefore, we build the indices to efficiently retrieve the counts 
%our indices store raw frequency counts 
%needed to compute \emph{smoothed} $n$gram probabilities using Kneser-Kney (KN) smoothing, 
%a powerful technique with elegant statistical properties\footnote{Although 
%we focus on KN-smoothing in this paper, our approach can be used to index and retrieve 
%counts needed in other smoothing techniques as well [chen and goodman].} [kneser kney].  
 
We make use of recent advances in 
\emph{compressed suffix trees} (\CSTs) \cite{nv-csurv07} 
to build compact indices  
with reasonable query-time (\S\ref{sec-suffix}) 
for high-order  LMs and investigate their behaviour 
with Kneser-Kney (KN) smoothing,  
a powerful technique with elegant statistical properties\footnote{Although
we focus on KN-smoothing, our approach can be used to index and compute
counts needed in other smoothing techniques as well \cite{chen1996empirical}.}  
\cite{kneser1995improved}.
% 
The key quantities for computing probability under KN-LMs
are raw frequencies of \ngrams 
and occurrence counts, i.e. the number of different contexts 
in which an \ngram has 
occurred which is challenging to compute.
%
%The raw frequency counts are simple to compute, but occurrence counts 
%are challenging. 
%
Our first proposal consists of two \CSTs to compute count queries 
in KN-LM (\S\ref{sec-lmsdsl}). 
% 
We further show how to leverage count queries for shorter contexts in queries for 
longer contexts to speed up the computation of KN-smoothed probabilities on the fly 
(\S\ref{sec-dual-cst}).
%
Our second proposal consists of only one \emph{augmented} \CST, which 
further improves both the space and query-time of our first proposal (\S\ref{sec-single-cst}). 
% 

Experiments ....
% Trevor: the experiments show that this approach matches the results of SRILM, while consuming a static memory footprint and construction cost irrespective of the \ngram order
% Trevor: futher, this allows us to run with unlimited markov order, which while slower than optimised low order models like \SRILM augurs well for future developments.
% Trevor: illustrate the scaling benefits of the model by building a character level LM over wikipedia, in only a few hours and modest memory footprint of a few GB
% Trevor: this work serves as a proof of concept, namely that high order language modelling is possible, and suitable for further optimisation to yield transformative tools for natural language processing.

%The predominant approach to count-based language modelling is the $n$gram model, where a typical LM can contain as many as several hundred billions of $n$grams XX.

%A typical LM can contain as many as severl hundred billions of $n$gramsFor modern NLP systems 
%The predominanr approach in language modelling is the $n$gram model, where the probability of a sequence of words is decomposed based on the chain rule and
%approximated using the Markov assumption XX. 
%

%The dominant approach to language modelling is $n$gram  model, where 

%LMs important, critical aspects are speed and scale. One avenue of better LMs using NN; another
%better count based LMs to harness more data (e.g., stupid backoff). 

%We're the latter, but marrying large scale with KN smoothing (better method). Crux is how to compute
%the quantities (counts, occ counts). 

%Suffix trees / arrays / tries etc used in the past. Either precomputed (KLM/SRI) or on the fly (slow
%LREC paper). But doesn't scale. We propose compressed SA/ST methods, which scale to massive datasets.

%Brief outline of how the method works; using one or two suffix trees. Forward/backward search.

%Pata on experiments: pplx good; can scale to large n and massive corpora; adequate runtime
%performance, around 100x slower than SRILM. Reranking gains? Can we run with INFINITE order?
