
\paragraph{Kneser Ney Language Modelling}
%\section{Kneser Ney Language Modelling}
\label{sec-lm}

%Background, mathematical formulation.

Recall our problem of efficient \ngram language modeling backed by a corpus encoded in a succinct index. % in such a way that they can be stored using only a small memory footprint and accessed efficiently. 
Although our method is generally applicable to many LM variants, we focus on the Kneser-Ney LM~\cite{kneser1995improved}, specifically the interpolated variant described in \newcite{chen1996empirical}, which has been shown to outperform other $n$gram LMs and has become the de-facto standard.

Interpolated Kneser-Ney describes the conditional probability of a word $w_i$ conditioned on the context of $m-1$ preceding words, $w_{i-m+1}^{i-1}$, as 
\begin{align}
P(w_i |& w_{i-m+1}^{i-1})=\frac{\max\left[c(w^{i}_{i-m+1}) - D_m,0\right]}{c(w^{i-1}_{i-m+1})} \nonumber \\
& \hspace{-6mm} +\frac{D_m \nlplus{w^{i-1}_{i-m-1}\Bigcdot} }{c(w^{i-1}_{i-m+1})}  
\bar{P}(w_i | w_{i-m+2}^{i-1}) ,  
\label{eq:high}
\end{align}
where lower-order smoothed probabilities are defined recursively (for $1<k<m$) as 
\begin{align}
\! \bar{P}(w_i |& w_{i-k+1}^{i-1})
 = \frac{\max\left[\nlplus{\Bigcdot w_{i-k+1}^{i}} - D_k,0\right]}{\nlplus{\Bigcdot w_{i-k+1}^{i-1} \Bigcdot}} \nonumber \\
&+ \frac{D_k \nlplus{w_{i-k+1}^{i-1}\Bigcdot}}{\nlplus{\Bigcdot w_{i-k+1}^{i-1} \Bigcdot}} \bar{P}(w_i | w_{i-k+2}^{i-1}) \,  . \! \label{eq:mid}
\end{align}
In the above formula, $D_k$ is the $k$gram-specific discount parameter, and 
the \emph{occurrence count} 
\mbox{$\nlplus{\alpha\Bigcdot} = |\{w: c(\alpha w)>0\}|$} 
is the number of observed word types following the pattern $\alpha$; 
the occurrence counts $\nlplus{\Bigcdot\alpha}$ and $\nlplus{\Bigcdot\alpha\Bigcdot}$ 
are defined accordingly.
%The highest order probability estimate in (\ref{eq:high}) interpolates a discounted frequency estimate with a $m-1$ order probability, defined as 
%Note the difference between (\ref{eq:mid})~and~(\ref{eq:high}), in that the frequency counts are redefined as occurrence counts. 
%The above equation is self-recursive, where each step shrinks the conditioning context by the least recent symbol. 
The recursion stops at unigram level where the unigram probabilities are defined as
%\begin{align*}
%\label{eq:low}
%
$ \bar{P}(w_i) = \nlplus{\Bigcdot w_i} / \nlplus{\Bigcdot\Bigcdot}$.%
\footnote{Modified Kneser-Ney, proposed
  by~\newcite{chen1996empirical}, typically outperforms interpolated
  Kneser-Ney through its use of context-specific discount parameters.
The implementation of this with our data structures is straightforward in principle, but brings a few added complexities in terms of dynamic computing other types of occurrence counts, which we leave for future work.}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "cstlm"
%%% End: 
